{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e661b934",
   "metadata": {},
   "source": [
    "## **Natural Lunguage Processing Project (NLP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03375e",
   "metadata": {},
   "source": [
    "**Business Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee49f1",
   "metadata": {},
   "source": [
    "* This analysis will aim to build a model that can rate the sentiment of a Tweet based on its content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f48503",
   "metadata": {},
   "source": [
    "**Objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad04917",
   "metadata": {},
   "source": [
    "* To build a multimodal classifier that will accurately classify tweets into positive, negative and neutral\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ec67f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from collections import  Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb4e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1015ad",
   "metadata": {},
   "source": [
    "* **Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b9056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b01857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking on the tail and head\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b330371",
   "metadata": {},
   "source": [
    "* **Sampling the dataset to have a random glimpse of the tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c607ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35195638",
   "metadata": {},
   "source": [
    "#### **DATA UNDERSTANDING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d4345",
   "metadata": {},
   "source": [
    "* Below function checks data information, value_counts, null_values in the corpus, columns and statitical description of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "328144de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def data_understanding(tweets):\n",
    "    # Shape of the dataset\n",
    "    display(f\"The shape of the dataset is: {data.shape}\")\n",
    "    print('*'*69, '\\n')\n",
    "\n",
    "    # Basic info abt the dataframe\n",
    "    data.info()\n",
    "    print('*'*69, '\\n')\n",
    "\n",
    "    # Value count in the target column\n",
    "    emotion_count = data['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()\n",
    "    print(emotion_count, '\\n')\n",
    "    print('*'*69)\n",
    "\n",
    "    # Check for Null\n",
    "    print(\"Null Values\", '\\n')\n",
    "    display(data.isnull().sum())\n",
    "    print('*'*69, '\\n')\n",
    "\n",
    "    # Columns present\n",
    "    print(\"Available columns\", '\\n')\n",
    "    display(data.columns)\n",
    "\n",
    "    display(data.describe())\n",
    "\n",
    "data_understanding(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e586467",
   "metadata": {},
   "source": [
    "### **Data preprocessing and cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707e674",
   "metadata": {},
   "source": [
    "* This step entails dowloading stopwords, plukt and wordnet from nltk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b306af",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d28a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ea6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce9601a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4') #Open Multilingual Wordnet (OMW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e60629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data for stopwords and punkt (tokenization)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01be110",
   "metadata": {},
   "source": [
    "* The function below performs conversion to lowercase,removal of htmls, hrefs, punctuations, tokenize the texts, join the tokensand also lematize the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1494f886",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load NLTK stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Check if the text is not NaN (handles missing values)\n",
    "    if not pd.isna(text):\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove HTML tags\n",
    "        text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Handle Mentioned Usernames\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "        # Remove non-alphanumeric characters (including numbers)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "        # Remove extra whitespaces and tokenize\n",
    "        text = ' '.join(word_tokenize(text))\n",
    "        \n",
    "#         # Apply spaCy NER\n",
    "#         doc = data(tweet_text)\n",
    "#         entities = ' '.join([ent.text for ent in doc.ents])\n",
    "\n",
    "        # Remove stopwords and lemmatize\n",
    "        text = ' '.join(lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the preprocessing function to the 'tweet_text' column in your DataFrame 'data'\n",
    "data['tweet_text'] = data['tweet_text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "988a1059",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83938a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "data.rename(columns={'tweet_text': 'tweets', 'emotion_in_tweet_is_directed_at': 'brand', 'is_there_an_emotion_directed_at_a_brand_or_product': 'emotion'}, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f194db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking on the null values in the columns\n",
    "data[['brand','tweets','emotion']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35663414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking on duplicates\n",
    "\n",
    "data['tweets'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a05f4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the null values by mapping the tweets with brand names to its brand in the brand column\n",
    "\n",
    "keywords = ['google', 'apple', 'ipad', 'android', 'iphone','samsung','sony']\n",
    "\n",
    "def find_brand(text):\n",
    "    if isinstance(text, str):\n",
    "        lower_text = text.lower()\n",
    "        for keyword in keywords:\n",
    "            if keyword in lower_text:\n",
    "                return keyword\n",
    "    return None\n",
    "\n",
    "data['brand'] = data.apply(lambda row: row['brand'] if not pd.isna(row['brand']) else find_brand(row['tweets']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccbede12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80cb9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rechecking the null values\n",
    "data['brand'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ecd55bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the null values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "119a9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rechecking for the null values\n",
    "data['brand'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff1d1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tweets\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "031fc362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the duplicated tweets\n",
    "data.drop_duplicates(subset=['tweets'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d274379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechecking the tweets for duplicates\n",
    "duplicated_tweets= data['tweets'].duplicated().sum()\n",
    "duplicated_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "963e2b88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1b21026",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Replace 'sxsw' with a space\n",
    "data['tweets'] = data['tweets'].str.replace('sxsw', ' ', case=False)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e0696",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dfce905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'data' is your DataFrame with a column named 'emotion'\n",
    "sentiment_counts = data['emotion'].value_counts()\n",
    "\n",
    "# Data\n",
    "categories = sentiment_counts.index\n",
    "sentiment_values = sentiment_counts.values\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(categories, sentiment_values, color=['lightgreen', 'salmon', 'lightgray'])\n",
    "\n",
    "# Title & Labels\n",
    "plt.title('Distribution of Sentiments', fontweight='bold')\n",
    "plt.xlabel('Sentiment Category', fontweight='bold')\n",
    "plt.ylabel('Count', fontweight='bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fccccd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from textwrap import wrap\n",
    "\n",
    "normal_words =' '.join([text for text in data['tweets']])\n",
    "                        \n",
    "# Generate the word cloud with the specified font path\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebf245dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking on unique products\n",
    "\n",
    "data[['tweets', 'brand', 'emotion']] = data[['tweets', 'brand', 'emotion']].apply(lambda x: x.str.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "208e4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.brand.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6904d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing other names\n",
    "\n",
    "data['brand'] = data['brand'].str.replace('ipad or iphone app', 'iphone')\n",
    "data['brand'] = data['brand'].str.replace('other google product or service', 'google')\n",
    "data['brand'] = data['brand'].str.replace('other apple product or service', 'iphone')\n",
    "data['brand'] = data['brand'].str.replace('apple', 'iphone')\n",
    "data['brand'] = data['brand'].str.replace('ipad', 'iphone')\n",
    "# data['brand'] = data['brand'].str.replace([['android','android app']], 'other brands')\n",
    "# data['brand'] = data['brand'].str.replace('android app','other brands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2c9a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.brand.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ec907b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grouping the dataframe by product and the emotions associated with each individual brand/company\n",
    "\n",
    "gb = data.groupby('brand')['emotion'].value_counts()\n",
    "\n",
    "# Creating the barplot\n",
    "\n",
    "gb.unstack(level=1).plot(kind='bar', \n",
    "                         figsize = (10,7), \n",
    "                         fontsize = 13, \n",
    "                         rot = 75,\n",
    "                         ylabel = 'Frequency',\n",
    "                         xlabel = 'brand/Company',\n",
    "                         title = 'Tweet Emotion Frequency by Product',\n",
    "                         colormap = 'Set2'\n",
    "                        )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "deffabfa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Brand mentions in the tweets\n",
    "\n",
    "c_freq = sns.barplot(x = data['brand'].value_counts(normalize = True).index, \n",
    "                    y = data['brand'].value_counts(normalize = True).values,\n",
    "                    order = ['iphone', 'google', 'android',''],\n",
    "                    palette = 'Set2'\n",
    "                    )\n",
    "c_freq.set(xlabel = 'Company', \n",
    "           ylabel = 'Relative Frequency',\n",
    "           title = 'Company Mention in Tweet Frequency'\n",
    "          )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1eb9f55f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41376fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5836d601",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking frequency of tokens from the tweets\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming 'tweet_text' column has been preprocessed\n",
    "preprocessed_tokens = ' '.join(data['tweets']).split()\n",
    "\n",
    "# Count the frequency of each token\n",
    "token_frequency = Counter(preprocessed_tokens)\n",
    "\n",
    "# Display the most common tokens and their frequencies\n",
    "token_frequency.most_common(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "684be929",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting top 20 most common tokens\n",
    "\n",
    "top_20_tokens= token_frequency.most_common(20)\n",
    "\n",
    "# Extract tokens and frequencies\n",
    "tokens, frequencies = zip(*top_20_tokens)\n",
    "\n",
    "# Plotting the horizontal bar graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(tokens, frequencies, color='skyblue')\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 20 Most Common Tokens')\n",
    "plt.gca().invert_yaxis()  # To have the highest frequency at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0445b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#defining a function for wordcloud generation\n",
    "def generate_wordcloud(tweets, collocations=False, background_color='black', \n",
    "                       colormap='Greens', display=True):\n",
    "    \n",
    "    ## Initalize a WordCloud\n",
    "    wordcloud = WordCloud(collocations=collocations, \n",
    "                          background_color=background_color, \n",
    "                          colormap=colormap, \n",
    "                          width=500, height=300)\n",
    "\n",
    "    ## Generate wordcloud from tokens\n",
    "    wordcloud.generate(','.join(tokens))\n",
    "\n",
    "    ## Plot with matplotlib\n",
    "    if display:\n",
    "        plt.figure(figsize = (12, 15), facecolor = None) \n",
    "        plt.imshow(wordcloud) \n",
    "        plt.axis('off');\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28a8bcec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#generating word cloud\n",
    "cloud_pos_w_company = generate_wordcloud(token_frequency.most_common(), collocations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3d55da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac490ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f171538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tweets_tagging(tweets):\n",
    "    tokens=word_tokenize(tweets)\n",
    "    pos_tags=pos_tag(tokens)\n",
    "    return pos_tags\n",
    "\n",
    "#Applying the function in tweets\n",
    "data[\"pos_tags\"]=data[\"tweets\"].apply(tweets_tagging)\n",
    "\n",
    "print(data[[\"tweets\",\"pos_tags\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "854dedd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a list to store named entities for each tweet\n",
    "named_entities_list = []\n",
    "\n",
    "# Process each tweet individually\n",
    "for tweet in data[\"tweets\"]:\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Extract named entities for each tweet\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    named_entities_list.append(named_entities)\n",
    "\n",
    "# Print or use the list as needed\n",
    "print(named_entities_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9544d1",
   "metadata": {},
   "source": [
    "**Polarity score using Vader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c54b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sia= SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = data[\"tweets\"]\n",
    "res = []\n",
    "\n",
    "for i, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    mtweet = row[\"tweets\"]\n",
    "    score = sia.polarity_scores(mtweet)\n",
    "    res.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8375b0af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Converting into dataframe\n",
    "vaders=pd.DataFrame(res)\n",
    "vaders.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d531237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the palarities with the metadata\n",
    "# merged_df = vaders.merge(data, on=\"tweets\", how=\"left\")\n",
    "\n",
    "# print(merged_df)\n",
    "merged_df = vaders.merge(data, left_index=True, right_index=True, how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22418b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax=sns.barplot(data=vaders, x =merged_df['brand'], y='compound')\n",
    "ax.set_title(\"polarity scores plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b078e22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Positive sentiment polarities\n",
    "import seaborn as sns\n",
    "ax=sns.barplot(data=vaders, x =merged_df['brand'], y='pos')\n",
    "ax.set_title(\"polarity scores plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b3935",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Negative sentiment polarities\n",
    "import seaborn as sns\n",
    "ax=sns.barplot(data=vaders, x =merged_df['brand'], y='neg')\n",
    "ax.set_title(\"polarity scores plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b5074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neutral sentiment polarities\n",
    "import seaborn as sns\n",
    "ax=sns.barplot(data=vaders, x =merged_df['brand'], y='neu')\n",
    "ax.set_title(\"polarity scores plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee04f53",
   "metadata": {},
   "source": [
    "#### FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c44915",
   "metadata": {},
   "source": [
    "#### TF-IDF (Term Frequency-Inverse Document Frequency):\n",
    "\n",
    "Convert text data into numerical vectors, emphasizing words that are unique to particular documents.\n",
    "##### Word2Vec:\n",
    "\n",
    "Generate dense vector representations of words, capturing semantic meanings based on contextual usage.\n",
    "##### Doc2Vec:\n",
    "\n",
    "Generate vector representations for entire documents or sentences, capturing the contextual and semantic information where understanding the semantic context of entire texts is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f13e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorization\n",
    "vectorizer            =  TfidfVectorizer()\n",
    "data_vec =  vectorizer.fit_transform(data['tweets']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5865f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661327cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vec_df = pd.DataFrame(data_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f8db3",
   "metadata": {},
   "source": [
    "####  Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f6e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Train Word2Vec model with a fixed seed\n",
    "word2vec_model = Word2Vec(sentences=data['tweets'], vector_size=100, window=5, min_count=1, workers=4, seed=seed)\n",
    "\n",
    "# Function to calculate the vector for a document by averaging the vectors of the words in the document\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
    "    \n",
    "    # If no words of the doc are in the word2vec vocab, return a vector of zeros\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    \n",
    "    # Otherwise, get the mean of the vectors\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "\n",
    "# Apply function to calculate vectors for all documents\n",
    "word2vec_vectors = data['tweets'].apply(lambda x: document_vector(word2vec_model, x))\n",
    "word2vec_df = pd.DataFrame(word2vec_vectors.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e8a23",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c560dd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn==0.8.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678cdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ba10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for modeling\n",
    "def train_and_evaluate(model, X, y, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Train and evaluate a machine learning model.\n",
    "\n",
    "    :param model: model object\n",
    "    :param X: Features\n",
    "    :param y: Target variable\n",
    "    :param test_size: Fraction of dataset to be used as test set\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    # Splitting the data\n",
    "    X = word2vec_df  # Features (TF-IDF values)\n",
    "    y =  data['emotion']  # Target variable\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Check if the model is a classifier\n",
    "    if not is_classifier(model):\n",
    "        raise ValueError(\"Model is not a classifier\")\n",
    "\n",
    "    # Create a pipeline with a scaler, SMOTE, and the model\n",
    "    pipeline = Pipeline([\n",
    "        #('scaler', StandardScaler(with_mean=False)),  # Use with_mean=False to support sparse input\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Measure the training time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the training time\n",
    "    training_time = end_time - start_time\n",
    "    print(\"Training time:\", training_time, \"seconds\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Predictions\n",
    "    train_preds = pipeline.predict(X_train)\n",
    "    test_preds = pipeline.predict(X_test)\n",
    "\n",
    "    # Probability estimates for each class\n",
    "    train_preds_proba = pipeline.predict_proba(X_train)\n",
    "    test_preds_proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"Model Performance\")\n",
    "    print(\"------------------\")\n",
    "\n",
    "    # Train Metrics\n",
    "    train_accuracy = accuracy_score(y_train, train_preds)\n",
    "    train_f1 = f1_score(y_train, train_preds, average='weighted')\n",
    "    train_roc_auc = roc_auc_score(y_train, train_preds_proba, multi_class='ovr', average='weighted')\n",
    "\n",
    "    print(\"Train Accuracy: \", train_accuracy)\n",
    "    print(\"Train F1 Score: \", train_f1)\n",
    "    print(\"Train ROC-AUC Score: \", train_roc_auc)\n",
    "    print(\"\\nClassification Report (Train Data):\\n\", classification_report(y_train, train_preds))\n",
    "\n",
    "    # Test Metrics\n",
    "    test_accuracy = accuracy_score(y_test, test_preds)\n",
    "    test_f1 = f1_score(y_test, test_preds, average='weighted')\n",
    "    test_roc_auc = roc_auc_score(y_test, test_preds_proba, multi_class='ovr', average='weighted')\n",
    "\n",
    "    print(\"\\nTest Accuracy: \", test_accuracy)\n",
    "    print(\"Test F1 Score: \", test_f1)\n",
    "    print(\"Test ROC-AUC Score: \", test_roc_auc)\n",
    "    print(\"\\nClassification Report (Test Data):\\n\", classification_report(y_test, test_preds))\n",
    "\n",
    "    # Create a DataFrame to store the metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': [str(model)],\n",
    "        'Train_Accuracy': [train_accuracy],\n",
    "        'Train_F1': [train_f1],\n",
    "        'Train_ROC_AUC': [train_roc_auc],\n",
    "        'Test_Accuracy': [test_accuracy],\n",
    "        'Test_F1': [test_f1],\n",
    "        'Test_ROC_AUC': [test_roc_auc],\n",
    "        'Training_Time': [training_time]\n",
    "    })\n",
    "\n",
    "    return pipeline, metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54373791",
   "metadata": {},
   "source": [
    "#### SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd2516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Instantiate and train SVC\n",
    "model1 = SVC(decision_function_shape='ovo')\n",
    "trained_svm_model, svm_metrics = train_and_evaluate(model1, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845504d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
